{
  "name": "DEAP Dataset",
  "full_name": "Database for Emotion Analysis using Physiological Signals",
  "abbreviation": "DEAP",
  "source": "Queen Mary University of London",
  "url": "https://www.eecs.qmul.ac.uk/mmv/datasets/deap/",
  "year": 2012,
  "authors": [
    "Koelstra, S.",
    "M\u00fchl, C.",
    "Soleymani, M.",
    "Lee, J.S.",
    "Yazdani, A.",
    "Ebrahimi, T.",
    "Pun, T.",
    "Nijholt, A.",
    "Patras, I."
  ],
  "citation": "Koelstra, S., M\u00fchl, C., Soleymani, M., Lee, J.S., Yazdani, A., Ebrahimi, T., Pun, T., Nijholt, A., & Patras, I. (2012). Deap: A database for emotion analysis using physiological signals. IEEE transactions on affective computing, 3(1), 18-31.",
  "doi": "10.1109/T-AFFC.2011.15",
  "license": "Research use only",
  "purpose": "Emotion recognition from physiological signals including EEG",
  "description": "DEAP is a multimodal dataset for the analysis of human affective states. It contains EEG and peripheral physiological signals of 32 participants as they watched 40 one-minute long excerpts of music videos. Participants rated each video in terms of valence, arousal, dominance, and liking.",
  "experimental_protocol": {
    "paradigm": "Emotion induction",
    "stimuli": "40 music video excerpts (1 minute each)",
    "number_of_subjects": 32,
    "sessions_per_subject": 1,
    "trials_per_subject": 40,
    "trial_duration": "60 seconds",
    "rating_dimensions": [
      "Valence",
      "Arousal",
      "Dominance",
      "Liking"
    ],
    "rating_scale": "1-9 scale"
  },
  "recording_parameters": {
    "sampling_rate": "128 Hz",
    "channels": 32,
    "electrode_positions": "10-20 system",
    "reference": "Common average reference",
    "filtering": "4-45 Hz bandpass",
    "amplifier": "Biosemi ActiveTwo system",
    "electrodes": "Ag/AgCl electrodes"
  },
  "data_format": {
    "file_format": "MATLAB (.mat)",
    "data_structure": "Preprocessed EEG epochs",
    "events": "Video onset/offset markers",
    "labels": "Continuous emotion ratings",
    "file_size": "~1.5 GB total"
  },
  "subject_demographics": {
    "age_range": "19-37 years",
    "gender": "16 male, 16 female",
    "handedness": "Not specified",
    "health_status": "Healthy subjects",
    "bci_experience": "Not specified"
  },
  "preprocessing": {
    "baseline_correction": "Applied",
    "artifact_removal": "EOG correction applied",
    "filtering": "4-45 Hz bandpass",
    "epoching": "60-second epochs",
    "downsampling": "128 Hz"
  },
  "evaluation_metrics": {
    "cross_validation": "Subject-specific evaluation",
    "performance_measure": "Classification accuracy, regression metrics",
    "baseline_methods": "SVM, Random Forest, Neural Networks",
    "expected_performance": "60-80% accuracy for binary classification"
  },
  "usage_notes": {
    "difficulty": "Medium - requires emotion recognition knowledge",
    "recommended_tools": "MATLAB, Python, MNE-Python",
    "common_applications": "Emotion recognition, affective computing",
    "limitations": "Limited to 32 subjects, music video stimuli only"
  },
  "related_datasets": [
    "MAHNOB-HCI",
    "AMIGOS",
    "DREAMER"
  ],
  "paper_relevance": {
    "suitability_for_fractional_methods": "High - emotions involve non-local temporal dynamics",
    "comparison_baseline": "Standard emotion recognition methods",
    "expected_improvement": "Fractional methods may capture emotional state transitions",
    "validation_approach": "Subject-specific cross-validation with statistical significance testing"
  }
}